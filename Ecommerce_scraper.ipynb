{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ecommerce-scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnk39OxFwlDAQSwyerh8fQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashparikh02/ecommerce-scraper/blob/main/Ecommerce_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzNFWCtL79vN"
      },
      "source": [
        "Made by Yash Parikh, Aneesh Tekulapally and Ayo Oguntula as a proof of concept for the Farallon Case Study competition\n",
        "\n",
        "***This is just a proof of concept on how we would crawl through just one website. In particular, we will note that we can get more granular information than just whether or not the website uses bigCommerce, but we can also see what domains use bigCommerce, etc. if we were to implement this using BeautifulSoup. That is the next goal of this project, and something we will work towards in the future. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csYwPHox7IUe"
      },
      "source": [
        "#Handle Dependencies\n",
        "from urllib.request import urlopen\n",
        "import csv"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7th0vuAAANV-"
      },
      "source": [
        "In order to look at longitudal data from the HTML file, we need to look at certain dates/times of interest. <br> We can get as specific as the day/time. Our code actually returns the closest backup to that time.\n",
        "    \n",
        "In general, the format for the times is 'YYYYMMDD', where one can enter as much information as they have\n",
        "\n",
        "    Examples:\n",
        "    '20190307' is March 7th, 2019 \n",
        "    '2019' is January 1st, 2019 \n",
        "    '201903' is March 1 2019, etc\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FefcPXPoHFQt"
      },
      "source": [
        "#set the times of interest \n",
        "years = ['2016','2017','2018','2019','2020']\n",
        "company =  \"/https://www.skullcandy.com/\"\n",
        "bigCommerce = []\n",
        "shopify = []\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReHRMLm8B9Ws"
      },
      "source": [
        "Next, we have to take the internet backup of the site, get the HTML bytes as latin-1 (not UTF since it includes information from other languages), decode these, and then we can determine if the string 'bigcommerce' appears in the files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAV5ga2A7Qjg"
      },
      "source": [
        "\n",
        "for year in years:\n",
        "    url = \"https://web.archive.org/web/\" + year + company\n",
        "    page = urlopen(url)\n",
        "    html_bytes = page.read()\n",
        "    html = html_bytes.decode(\"latin-1\")\n",
        "    bigCommerce.append(int('bigcommerce' in html))\n",
        "    shopify.append(int('shopify' in html))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GirZSb6LCJgL"
      },
      "source": [
        "Let's make sure that we have the information present here\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4RJQpwnWjO7",
        "outputId": "0f5aea2b-b807-4c53-f384-91eb4d737238"
      },
      "source": [
        "print(years)\n",
        "print(bigCommerce)\n",
        "print(shopify)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2016', '2017', '2018', '2019', '2020']\n",
            "[0, 0, 1, 1, 1]\n",
            "[0, 0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFzb4mSCQt4"
      },
      "source": [
        "We next label our data, to be sure that we can read the information when we put it into a CSV file for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsCEu_Y_7XBK",
        "outputId": "582cb809-2eac-4c4b-c194-fc0fa09f1dd3"
      },
      "source": [
        "#combine information and get it ready to be put into a csv\n",
        "years.insert(0, company)\n",
        "bigCommerce.insert(0,'BigCommerce')\n",
        "shopify.insert(0,'Shopify')\n",
        "data = []\n",
        "\n",
        "data.append(years)\n",
        "data.append(bigCommerce)\n",
        "data.append(shopify)\n",
        "print(data)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['/https://www.skullcandy.com/', '2016', '2017', '2018', '2019', '2020'], ['BigCommerce', 0, 0, 1, 1, 1], ['Shopify', 0, 0, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paB4ZeAkCYgQ"
      },
      "source": [
        "Once that's done, we can add it to a CSV file. (Note: the CSV file will appear in the folder on the right-- it is not a file local to your computer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Moiae07n71OA",
        "outputId": "7fb3c2f4-405d-4673-c77d-77308cfcc827"
      },
      "source": [
        "print(data)\n",
        "with open('test.csv', 'w', newline = '') as f:\n",
        "    # create the csv writer\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(data)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['/https://www.skullcandy.com/', '2016', '2017', '2018', '2019', '2020'], ['BigCommerce', 0, 0, 1, 1, 1], ['Shopify', 0, 0, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KlM4Pil72Rn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}